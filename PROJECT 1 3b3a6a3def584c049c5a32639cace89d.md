# PROJECT 1

### memo

ê¸°íƒ€ ìŒí˜• ë°ì´í„°ì—ì„œ í´ë¦° í†¤ê³¼ ë‹¤ì–‘í•œ ì´í™í„°(ë“œë¼ì´ë¸Œ, ë¦¬ë²„ë¸Œ, ë”œë ˆì´, ëª¨ë“ˆë ˆì´ì…˜)ë¥¼ êµ¬ë¶„í•˜ë ¤ë©´ ê° ì´í™í„°ê°€ ì†Œë¦¬ì— ë¯¸ì¹˜ëŠ” íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ íŠ¹ì§•(feature)ì„ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ê° ì´í™í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ì£¼ìš” ì˜¤ë””ì˜¤ íŠ¹ì§•ë“¤ì…ë‹ˆë‹¤:

1. **ë©œ-ì£¼íŒŒìˆ˜ ì¼‘ìŠ¤íŠ¸ëŸ¼ ê³„ìˆ˜ (MFCCs)**: ì†Œë¦¬ì˜ ì§ˆê°ê³¼ íƒ€ì´ë°ì„ ì œì™¸í•œ ì£¼íŒŒìˆ˜ íŠ¹ì„±ì„ í¬ì°©í•˜ì—¬ ê° ì´í™í„°ì˜ ê³ ìœ í•œ ì£¼íŒŒìˆ˜ ì‘ë‹µì„ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. **ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ (Spectral Centroid)**: ì†Œë¦¬ì˜ "ë°ê¸°"ë‚˜ "ëª…ë£Œì„±"ì„ ì¸¡ì •í•˜ì—¬, íŠ¹íˆ ëª¨ë“ˆë ˆì´ì…˜ ì´í™í„°ì˜ ë³€ì¡° íš¨ê³¼ë¥¼ ê°ì§€í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
3. **ìŠ¤í™íŠ¸ëŸ´ ë¡¤ì˜¤í”„ (Spectral Rolloff)**: ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ì˜ ìƒìœ„ ë¶€ë¶„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì—¬, íŠ¹íˆ ë”œë ˆì´ì™€ ë¦¬ë²„ë¸Œì˜ ì”í–¥ íŠ¹ì„±ì„ í¬ì°©í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
4. **ì œë¡œ í¬ë¡œì‹± ë ˆì´íŠ¸ (Zero Crossing Rate, ZCR)**: ì˜¤ë””ì˜¤ ì‹ í˜¸ê°€ ì‹œê°„ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ìì£¼ ë¶€í˜¸ë¥¼ ë³€ê²½í•˜ëŠ”ì§€ ì¸¡ì •í•˜ì—¬, íŠ¹íˆ ë“œë¼ì´ë¸Œ ê°™ì€ ë””ìŠ¤í† ì…˜ ì´í™í„°ì˜ ì„±ê²©ì„ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
5. **ìŠ¤í™íŠ¸ëŸ´ í”Œë«ë‹ˆìŠ¤ (Spectral Flatness)**: ìŠ¤í™íŠ¸ëŸ¼ì´ ì–¼ë§ˆë‚˜ í‰í‰í•œì§€ë¥¼ ì¸¡ì •í•˜ì—¬, íŠ¹íˆ ë¦¬ë²„ë¸Œì™€ ê°™ì´ ì£¼íŒŒìˆ˜ ì‘ë‹µì„ ê· ì¼í•˜ê²Œ í•˜ëŠ” ì´í™í„°ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
6. **ìŠ¤í™íŠ¸ëŸ´ ì»¨íŠ¸ë¼ìŠ¤íŠ¸ (Spectral Contrast)**: ìŠ¤í™íŠ¸ëŸ¼ì˜ í”¼í¬ì™€ ë°¸ë¦¬ ì‚¬ì´ì˜ ëŒ€ë¹„ë¥¼ ì¸¡ì •í•˜ì—¬, ë‹¤ì–‘í•œ ì´í™í„°ê°€ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ êµ¬ë³„í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
7. **í¬ë¡œë§ˆ í”¼ì²˜ (Chroma Feature)**: ìŒì•…ì˜ ì¡°ì„±ì„ ë°˜ì˜í•˜ì—¬, íŠ¹íˆ ëª¨ë“ˆë ˆì´ì…˜ ì´í™í„°ì˜ ë³€ì¡° íš¨ê³¼ê°€ ì¡°ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<aside>
ğŸ’¡ ê¸°íƒ€ ì›ìŒê³¼ ì´í™íŒ… ì‚¬ìš´ë“œ ë¶„ë¥˜ ë° ë¨¸ì‹ ëŸ¬ë‹ / cnní›ˆë ¨ â†’ GANsynthí•™ìŠµìœ¼ë¡œ effecting sound ìƒì„±

</aside>

# ëª©í‘œ

1. clean toneê³¼ effecting sound ì‚¬ì´ì— ë¶„ë¥˜ê°€ ê°€ëŠ¥í•œ ìœ ì˜ë¯¸í•œ data ì°¾ê¸°
2. ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ clean toneê³¼ effecting sound ë¶„ë¥˜í•˜ê¸°

# VERSION

## 1-1

- code
    
    ```python
    import os
    import pickle
    import librosa
    import soundfile as sf
    import numpy as np
    from tqdm import tqdm
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import classification_report, accuracy_score
    from sklearn.impute import SimpleImputer
    
    #sound data ì»´í“¨í„°ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°, ê° íŒŒì¼ ë³„ ì´í™í„°, í”½ì—… ì¶”ì¶œ)
    def load_classify_audio_files(base_path):
    
      audio_data={}
      for root, dirs, files in os.walk(base_path):
        for file in files:
          if file.endswith((".wav", ".mp3")):
            path=os.path.join(root, file)
            parts=path.split(os.sep)
            file_name_no_ext  = os.path.splitext(parts[-1])[0]
            key=tuple(parts[-3:-1]+[file_name_no_ext])
            if key not in audio_data:
              audio_data[key]=[]
            data, sr = librosa.load(path, sr=None)
            audio_data[key].append((data, sr))
      return audio_data
    
    #sound data íŠ¹ì„± ì¶”ì¶œ
    def ext_features(data, sr):
        mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13), axis=1)
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=data, sr=sr))
        spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=data, sr=sr))
        zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=data))
        spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=data))
        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=data, sr=sr), axis=1)
        chroma_feature = np.mean(librosa.feature.chroma_stft(y=data, sr=sr), axis=1)
    
        features = {
            'mfcc': mfcc,
            'spectral_centroid': spectral_centroid,
            'spectral_rolloff': spectral_rolloff,
            'zero_crossing_rate': zero_crossing_rate,
            'spectral_flatness': spectral_flatness,
            'spectral_contrast': spectral_contrast,
            'chroma_feature': chroma_feature
        }
        return features
    
    #íŠ¹ì„± ì €ì¥
    features_file_path = 'features_by_key.pkl'
    
    user_input = input("Do you want to reset the data? (yes/no): ")
    
    if user_input.lower() == 'yes':
      if os.path.exists(features_file_path):
        os.remove(features_file_path)
        print("Data has been reset.")
      else:
        print("No data file found to reset.")
    
      base_path = '/Users/nyoung/Desktop/dev/project/1. guitar effecting sound encoder-decoder/sound samples'
      classified_audio_files = load_classify_audio_files(base_path)
    
      #íŒŒì¼ ë³„ íŠ¹ì„± ë°°ì¹˜
      features_by_key = {}
      for key, files in tqdm(classified_audio_files.items(), desc="Processing each effect configuration"):
        features_by_key[key]=[]
        for data, sr in tqdm(files, desc=f"Processing files for {key}", leave=False):
          features = ext_features(data, sr)
          features_by_key[key].append(features)
    
      with open('features_by_key.pkl', 'wb')as f:
        pickle.dump(features_by_key, f)
    
    #íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
    else:
      if os.path.exists(features_file_path):
        with open(features_file_path, 'rb') as f:
          features_by_key = pickle.load(f)
        print("Data loaded from existing file.")
      else:
        print("No existing data file found. Please reset data to proceed.")
    
    #label ì •ì˜
    effect_mapping = {
        'BluesDriver': 'drive',
        'Chorus': 'chorus',
        'Clean': 'clean',
        'Digital Delay': 'delay',
        'Flanger': 'flanger',
        'Hall Reverb': 'reverb',
        'Phaser': 'phaser',
        'Plate Reverb': 'reverb',
        'RAT': 'drive',
        'Spring Reverb': 'reverb',
        'Sweep Echo': 'delay',
        'TapeEcho': 'delay',
        'TubeScreamer': 'drive'
    }
    
    label_mapping = {
        'clean': 0,
        'drive': 1,
        'reverb': 2,
        'delay': 3,
        'chorus': 4,
        'phaser': 5,
        'flanger': 6
    }
    
    features_list = []
    labels_list = []
    file_names = []
    
    for key, features in features_by_key.items():
      effect_type = key[0]
      category = effect_mapping.get(effect_type, None)
      if category:
        label = label_mapping[category]
        for feature in features:
          features_list.append(feature)
          labels_list.append(label)
          file_names.append(key[2])
    
    #example
    '''
    reverb_label = label_mapping['reverb']
    reverb_features = [features_list[i] for i, label in enumerate(labels_list) if label == reverb_label]
    
    print(f"Number of 'reverb' features : {len(reverb_features)}")
    for feature in reverb_features[:5]:
      print(feature)
    '''
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(features_list, columns=[
        'MFCC', 'Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'Spectral Flatness', 'Spectral Contrast', 'Chroma Feature'])
    df['Label'] = labels_list
    
    # ë°ì´í„° ì „ì²˜ë¦¬: ìŠ¤ì¼€ì¼ë§ ë° NaN ê°’ ëŒ€ì²´
    scaler = StandardScaler()
    imputer = SimpleImputer(strategy='mean')
    
    X_imputed = imputer.fit_transform(df.drop('Label', axis=1))
    X_scaled = scaler.fit_transform(X_imputed)
    
    # ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, df['Label'], test_size=0.2, random_state=42)
    
    # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€
    y_pred = model.predict(X_test)
    print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))
    
    # SVM ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
    svm_model = SVC(kernel='linear', random_state=42)
    svm_model.fit(X_train, y_train)
    
    # SVM ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€
    svm_y_pred = svm_model.predict(X_test)
    print("SVM Accuracy:", accuracy_score(y_test, svm_y_pred))
    print(classification_report(y_test, svm_y_pred, target_names=label_mapping.keys()))
    
    # ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
    
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    
    # ìµœì  ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€
    best_y_pred = best_model.predict(X_test)
    print("Best Model Accuracy:", accuracy_score(y_test, best_y_pred))
    print(classification_report(y_test, best_y_pred, target_names=label_mapping.keys()))
    
    '''
    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    selected_xticks = ['1-0', '2-0', '3-0', '4-0', '5-0', '6-0']
    
    features = ['MFCC', 'Spectral Centroid', 'Spectral Rolloff', 'Zero Crossing Rate', 'Spectral Flatness', 'Spectral Contrast', 'Chroma Feature']
    plt.figure(figsize=(14, 30))
    
    for i, feature in enumerate(features, 1):
        plt.subplot(7, 1, i)
        sns.lineplot(data=df, x='File Name', y=feature, hue='Effect Type', palette='viridis')
        plt.title(feature)
        plt.xlabel('File Name (String-Fret)')
        plt.ylabel(feature)
        plt.xticks(selected_xticks, rotation=90)
        plt.legend(title='Effect Type', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True)
    
    plt.tight_layout()
    
    output_file_path = '/Users/nyoung/Desktop/dev/feature_plots.png'
    plt.savefig(output_file_path)
    
    plt.show()
    '''
    ```
    
- result
    
    ![feature_plots.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/feature_plots.png)
    
    ```
       Accuracy: 0.2246376811594203
       
               precision    recall  f1-score   support
    
       clean       0.00      0.00      0.00       146
       drive       0.00      0.00      0.00       415
      reverb       0.00      0.00      0.00       421
       delay       0.22      1.00      0.37       403
      chorus       0.00      0.00      0.00       149
      phaser       0.00      0.00      0.00       130
     flanger       0.00      0.00      0.00       130
    
    accuracy                           0.22      1794
    macro avg      0.03      0.14      0.05      1794
    weighted avg   0.05      0.22      0.08      1794
    ```
    
    (ã… ã… â€¦)
    

ì²˜ìŒ ì´ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•  ë•ŒëŠ” ë§‰ì—°íˆ fourier transformingì„ í†µí•´ì„œ êµ¬ë¶„ì„ í•´ë³´ì ìƒê°í–ˆìœ¼ë‚˜, ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•´ì„œëŠ” êµ¬ë¶„ì´ ê°€ëŠ¥í•œ ìœ ì˜ë¯¸í•œ ë°ì´í„°ê°€ ìˆì–´ì•¼í•œë‹¤ê³  ìƒê°í•´ì„œ ìŒí˜•ì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” 7ê°€ì§€ íŠ¹ì„±(MFCC, ZCR, Spectral Centroid, Spectral Rolloff, Zero Crossing Rate, Spectral Flatness, Spectral Contrast, Chroma Feature)ì„ ì¶”ì¶œí•˜ì—¬ êµ¬ë¶„í•˜ë ¤í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê° íŠ¹ì„±ìœ¼ë¡œ effecting soundë¥¼ êµ¬ë¶„í•˜ëŠ”ë°ì—ëŠ” ë¬¸ì œê°€ ìˆì—ˆë˜ ë“¯ ì‹¶ë‹¤. ê·¸ë˜í”„ë¥¼ í™•ì¸í•´ë´¤ì„ë•Œ, ëª‡ëª‡ featureì—ì„œ ëª‡ëª‡ effectsëŠ” ìœ ì‚¬í•œ graphê°œí˜•ì„ ê°€ì§€ë©°, yì¶•ì˜ ìœ„ìƒ ì°¨ì´ë¡œ ì„œë¡œë¥¼ êµ¬ë¶„í• ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ìœ¼ë‚˜, ê³µí†µì ìœ¼ë¡œ chorus, flangerì—ì„œ(4,6ë²ˆ) graphê°€ íŠ€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. (ê·¸ë ‡ë‹¤ê³  ë‹¤ë¥¸ effectê°€ ì•ˆíŠ€ëŠ” ê²ƒë„ ì•„ë‹ˆë‹¤â€¦ã… ã… )ì™œ ê·¸ëŸ°ê°€ ìƒê°ì„ í•´ë³´ë©´ chorus, flangerê³„ì—´ì€ ì›ìŒì˜ ìŒì •ì„ ì‚´ì§ ë³€í˜•ì‹œì¼œ ì›ìŒê³¼ í•¨ê»˜ ì¬ìƒì‹œí‚¤ëŠ” í˜•íƒœì¸ë° ê·¸ëŸ¬ë©´ì„œ frequencyê°€ ë§ì´ ì„ì¸ë“¯ í•˜ë‹¤. ì´ë²ˆì—” ì›ë˜ ìƒê°ëŒ€ë¡œ fftë¥¼ í†µí•´ ì°¨ì´ì ì„ ì°¾ì•„ë³´ì

## 1-2-1

- code
    
    ```python
    import os
    import pickle
    import numpy as np
    import pandas as pd
    import librosa
    import matplotlib.pyplot as plt
    from scipy.fft import fft
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, accuracy_score
    from sklearn.preprocessing import StandardScaler
    from tqdm import tqdm
    
    #íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° + íŒŒì¼ë³„ ì´í™í„°, í”½ì—… ì¶”ì¶œ
    def load_classify_files(base_path):
      audio_data =  {}
      for root, dirs, files in os.walk(base_path):
        for file in files:
          if file.endswith((".wav", ".mp3")):
            path = os.path.join(root, file)
            parts = path.split(os.sep)
            file_no_ext = os.path.splitext(parts[-1])[0]
            key = tuple(parts[-3:-1]+[file_no_ext])
            if key not in audio_data:
              audio_data[key]=[]
            data, sr = librosa.load(path, sr=None)
            audio_data[key].append((data, sr))
      return audio_data
    
    #FFT feature ì¶”ì¶œ
    def ext_fft_features(data, sr, n_fft=2048):
      fft_vals = np.abs(fft(data, n=n_fft)[:n_fft//2])
      return fft_vals
    
    #feature ì €ì¥
    features_file_path = 'features_by_key.pkl'
    
    user_input = input("Do you want to reset the data? (yes/no): ")
    
    if user_input.lower()=='yes':
      if os.path.exists(features_file_path):
        os.remove(features_file_path)
        print("Data has been reset.")
      else:
        print("No data file found to reset.")
    
      base_path = '/Users/nyoung/Desktop/dev/project/1. guitar effecting sound encoder-decoder/sound samples'
      classified_audio_files = load_classify_files(base_path)
    
      #fileë³„ feature ë°°ì¹˜
      features_by_key = {}
      for key, files in tqdm(classified_audio_files.items(), desc="Processing each effect configuration"):
        features_by_key[key]=[]
        for data, sr in tqdm(files, desc=f"Processing files for {key}", leave=False):
          fft_features = ext_fft_features(data, sr)
          features_by_key[key].append(fft_features)
    
      with open(features_file_path, 'wb') as f:
        pickle.dump(features_by_key, f)
    
    # file ë¶ˆëŸ¬ì˜¤ê¸°
    else:
        if os.path.exists(features_file_path):
            try:
                with open(features_file_path, 'rb') as f:
                    features_by_key = pickle.load(f)
                print("Data loaded from existing file.")
            except EOFError:
                print("Error: File is empty or corrupted. Please reset the data.")
                features_by_key = {}
        else:
            print("No existing data file found. Please reset data to proceed.")
            features_by_key = {}
    
    #data ì¤€ë¹„
    features_list = []
    labels_list = []
    file_names  = []
    
    effect_mapping = {
      'BluesDriver': 'drive',
      'Chorus': 'chorus',
      'Clean': 'clean',
      'Digital Delay': 'delay',
      'Flanger': 'flanger',
      'Hall Reverb': 'reverb',
      'Phaser': 'phaser',
      'Plate Reverb': 'reverb',
      'RAT': 'drive',
      'Spring Reverb': 'reverb',
      'Sweep Echo': 'delay',
      'TapeEcho': 'delay',
      'TubeScreamer': 'drive'
    }
    
    label_mapping = {
      'clean': 0,
      'drive': 1,
      'reverb': 2,
      'delay': 3,
      'chorus': 4,
      'phaser': 5,
      'flanger': 6
    }
    
    for key, features in tqdm(features_by_key.items(), desc="Preparing data for modeling"):
      effect_type = key[0]
      category = effect_mapping.get(effect_type, None)
      if category in label_mapping:
        for feature in features:
          features_list.append(feature)
          labels_list.append(label_mapping[category])
          file_names.append(key[2])
    
    # graph ê·¸ë¦¬ê¸°
    '''
    output_dir = '/Users/nyoung/Desktop/dev/project/1. guitar effecting sound encoder-decoder/fft_plots'
    os.makedirs(output_dir, exist_ok=True)
    
    def plot_fft_features(features_by_key, effect_mapping, output_dir):
        num_effects = len(effect_mapping) - list(effect_mapping.values()).count('ignore')
        fig, axs = plt.subplots(num_effects, 1, figsize=(15, 2 * num_effects))
        
        effect_index = 0
        for effect, category in effect_mapping.items():
            if category == 'ignore':
                continue
            for key, features in features_by_key.items():
                if key[0] == effect:
                    axs[effect_index].plot(features[0])
                    axs[effect_index].set_title(f'FFT Frequency Features for {effect}')
                    axs[effect_index].set_xlabel('Frequency Bin')
                    axs[effect_index].set_ylabel('Amplitude')
                    axs[effect_index].legend([effect])
                    break
            effect_index += 1
    
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'all_effects_fft_plot.png'))
        plt.show()
    
    plot_fft_features(features_by_key, effect_mapping, output_dir)
    '''
    
    #Data frame ìƒì„±
    df = pd.DataFrame(features_list)
    df['Label'] = labels_list
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df.drop('Label', axis=1))
    
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, df['Label'], test_size=0.2, random_state=42)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))
    ```
    
- result
    
    ![all_effects_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/all_effects_fft_plot.png)
    
    ![Clean_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Clean_fft_plot.png)
    
    ![BluesDriver_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/BluesDriver_fft_plot.png)
    
    ![RAT_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/RAT_fft_plot.png)
    
    ![TubeScreamer_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/TubeScreamer_fft_plot.png)
    
    ![Digital Delay_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Digital_Delay_fft_plot.png)
    
    ![Sweep Echo_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Sweep_Echo_fft_plot.png)
    
    ![TapeEcho_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/TapeEcho_fft_plot.png)
    
    ![Hall Reverb_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Hall_Reverb_fft_plot.png)
    
    ![Plate Reverb_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Plate_Reverb_fft_plot.png)
    
    ![Spring Reverb_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Spring_Reverb_fft_plot.png)
    
    ![Chorus_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Chorus_fft_plot.png)
    
    ![Phaser_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Phaser_fft_plot.png)
    
    ![Flanger_fft_plot.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Flanger_fft_plot.png)
    
    ```
    Random Forest Accuracy: 0.6917502787068004
                precision    recall  f1-score   support
       clean       0.40      0.01      0.03       146
       drive       0.88      0.84      0.86       415
      reverb       0.79      0.98      0.87       421
       delay       0.57      0.97      0.72       403
      chorus       0.39      0.08      0.13       149
      phaser       0.49      0.18      0.27       130
     flanger       0.50      0.40      0.44       130
    
    accuracy                           0.69      1794
    macro avg      0.57      0.50      0.47      1794
    weighted avg   0.65      0.69      0.63      1794
    ```
    

ì¡°ê¸ˆì€ ì •í™•ë„ê°€ ì˜¬ëë‹¤..! FFTì˜ frequency graphë¥¼ ê° effectorë³„ë¡œ í•œ ê°œ ì”© ë½‘ì•„ë´¤ì„ë•Œë„ ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìœ ì˜ë¯¸í•œ ê³µí†µì ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ clean, modulation ì¹´í…Œê³ ë¦¬ê°€ ì „ì²´ì ìœ¼ë¡œ ì •í™•ë„ê°€ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆëŠ”ë°(íŠ¹íˆ recallê°’ì´ ë„ˆë¬´ë‚®ë‹¤â€¦)ê·¸ê±´ ì•„ë§ˆë„ dataê°œìˆ˜ê°€ drive, reverb, delayë³´ë‹¤ 3ë°° ê°€ê¹Œì´ ì‘ê¸° ë•Œë¬¸ì— ê·¸ëŸ° ê²ƒ ê°™ë‹¤. ì•„ë§ˆ ë°ì´í„° ì¦ê°•ì´ ì¡°ê¸ˆ í•„ìš”í•œ ìƒí™©ì´ ì•„ë‹ê¹Œ ìƒê°í•´ë³¸ë‹¤.

1. ê´œì°®ì€ dataë¥¼ ì°¾ì•˜ë‹¤! ì‚¬ì‹¤ ì§€ê¸ˆ í•™ìŠµì— ì“°ì´ëŠ” dataì˜ ìˆ˜ëŠ” ê° effetorë§ˆë‹¤ ëŒ€ëµ 100ê°œ ë°–ì— ì•ˆë˜ì–´ì„œ í•™ìŠµì„ í•  ë•Œ ì–´ëŠì •ë„ ë¶€ì¡±í•œ ì ì´ ìˆì—ˆëŠ”ë°, ì´ë²ˆ êº¼ëŠ” ê° effectorë§ˆë‹¤ 1000ê°œì”© ìˆë‹¤ ì¢…ë¥˜ë„ ë‹¤ì–‘í•œë“¯(ì••ì¶•íŒŒì¼ì´ 6.4Gë¼ ë°›ëŠ” ì‹œê°„ì´ ì—„ì²­ë‚˜ê²Œ ê±¸ë¦¬ëŠ”ê²ƒì´ ë‹¨ì ì´ë¼ë©´ ë‹¨ì ì´ë‹¤â€¦) ì´ê±¸ ì¶”ê°€ë¡œ í•™ìŠµí•´ë´ì•¼í•  ë“¯)
    
    [IDMT-SMT-Audio-Effects Dataset](https://zenodo.org/records/7544032)
    
2. ì¶”ê°€ì ìœ¼ë¡œ ì‹œê°„ì— ë”°ë¥¸ FFT dataë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ”ì§€ë„ ì•Œì•„ë´ì•¼í•  ê²ƒ ê°™ë‹¤. â†’ STFT ì¨ë³´ê¸°
    
    [STFT(Short-Time Fourier Transform)ì™€ Spectrogramì˜ pythonêµ¬í˜„ê³¼ ì˜ë¯¸](https://sanghyu.tistory.com/37)
    

## 1-2-2

ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ìê¾¸ë§Œ ì•„ë˜ì™€ ê°™ì€ ì˜¤ë¥˜ê°€ ë‚˜ë©´ì„œ kill ë˜ì—ˆë‹¤â€¦

```jsx
Processing each effect configuration:  91%|â–‰| 37657/41334 [04:
zsh: killed     /usr/bin/python3 7658):   0%| | 0/1 [00:00<?, ?i
(base) nyoung@iyun-yeong-ui-MacBookPro ~ % 
/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: 
There appear to be 1 leaked semaphore objects to clean up at shutdown
warnings.warn('resource_tracker: There appear to be %d '
```

ì™¤ê¹Œâ€¦.í•˜ë©° ì¸í„°ë„·ì— ì°¾ì•„ë´¤ì„ ë•Œì—ë„ ëª…ì¾Œí•œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆë‹¤â€¦

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-22 á„‹á…©á„’á…® 5.07.58.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-22_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.07.58.png)

~~(ì—­ì‹œ í˜•ì´ì•¼ êµ¬í•´ì£¼ëŸ¬ ì™”êµ¬ë‚˜â€¦.)~~

ë‚´ ì¹œêµ¬ gptëŠ” 

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-05-22 á„‹á…©á„’á…® 5.09.20.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-22_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_5.09.20.png)

ë¼ê³  í•´ì„œ ì¼ë‹¨ ì´ë ‡ê²Œ í•´ë³¼ê¹Œ í•œë‹¤.

ì²˜ë¦¬ ì™„ë£Œ batchì— ëŒ€í•´ì„œ ë°°ì› ë‹¤.â†’ ì‹¤íŒ¨

- í•™ìŠµ ë‚´ìš©
    
    ### **ì»´í“¨í„° ë©”ëª¨ë¦¬ êµ¬ì¡°**
    
    1. **ì£¼ ë©”ëª¨ë¦¬(RAM)**:
        - ì£¼ ë©”ëª¨ë¦¬ëŠ” ì»´í“¨í„°ê°€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” ì„ì‹œ ì €ì¥ ê³µê°„ì…ë‹ˆë‹¤. ë¹ ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆì§€ë§Œ ìš©ëŸ‰ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
        - í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ë  ë•Œ í•„ìš”í•œ ë°ì´í„°ì™€ ì½”ë“œê°€ ì£¼ ë©”ëª¨ë¦¬ì— ë¡œë“œë©ë‹ˆë‹¤.
    2. **ê°€ìƒ ë©”ëª¨ë¦¬**:
        - ì£¼ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ, í•˜ë“œ ë””ìŠ¤í¬ì˜ ì¼ë¶€ë¥¼ ë©”ëª¨ë¦¬ì²˜ëŸ¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê°€ìƒ ë©”ëª¨ë¦¬ë¼ê³  í•©ë‹ˆë‹¤. ì´ ì˜ì—­ì„ ìŠ¤ì™‘ ì˜ì—­(swap space)ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤.
        - ê°€ìƒ ë©”ëª¨ë¦¬ëŠ” ì‹¤ì œ RAMë³´ë‹¤ ì ‘ê·¼ ì†ë„ê°€ ëŠë¦¬ê¸° ë•Œë¬¸ì—, ê°€ìƒ ë©”ëª¨ë¦¬ë¥¼ ê³¼ë„í•˜ê²Œ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.
    
    ### **ë°°ì¹˜ ì²˜ë¦¬ê°€ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì´ìœ **
    
    1. **ë©”ëª¨ë¦¬ ì‚¬ìš© ê°ì†Œ**:
        - í•œ ë²ˆì— ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë©´, í•´ë‹¹ ë°ì´í„°ë¥¼ ëª¨ë‘ ì£¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ ë©”ëª¨ë¦¬ ìš©ëŸ‰ì„ ì´ˆê³¼í•˜ê²Œ ë˜ì–´ ê°€ìƒ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ì„±ëŠ¥ ì €í•˜ì™€ í•¨ê»˜ ì‹œìŠ¤í…œì˜ ë¶ˆì•ˆì •ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ë°ì´í„°ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ë©´, í•œ ë²ˆì— ì²˜ë¦¬í•´ì•¼ í•  ë°ì´í„°ì˜ ì–‘ì´ ì¤„ì–´ë“¤ì–´ ì£¼ ë©”ëª¨ë¦¬ì˜ ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“­ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì£¼ ë©”ëª¨ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. **ìºì‹œ íš¨ìœ¨ì„±**:
        - CPUì—ëŠ” ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ìºì‹œ ë©”ëª¨ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•´ ì‘ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´, í•´ë‹¹ ë°ì´í„°ê°€ CPU ìºì‹œì— ë” ì˜ ì ì¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°ì´í„° ì ‘ê·¼ ì†ë„ë¥¼ ë†’ì´ê³  ì „ì²´ ì²˜ë¦¬ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    3. **ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê´€ë¦¬**:
        - íŒŒì´ì¬ê³¼ ê°™ì€ ì–¸ì–´ì—ì„œëŠ” ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•˜ëŠ” ê°€ë¹„ì§€ ì»¬ë ‰í„°ê°€ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´, ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì´ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ê³  í”„ë¡œê·¸ë¨ì˜ ì•ˆì •ì„±ì„ ë†’ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
    
    ### **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**
    
    1. **CPU ì‚¬ìš©ëŸ‰**:
        - í•œ ë²ˆì— ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë ¤ê³  í•˜ë©´, CPUê°€ ê³¼ë¶€í•˜ì— ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ëŠ” ì‘ì—…ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ CPUê°€ ê³¼ë¶€í•˜ ì—†ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    2. **ë””ìŠ¤í¬ I/O**:
        - ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì €ì¥í•˜ëŠ” ê³¼ì •ì—ì„œ ë””ìŠ¤í¬ ì…ì¶œë ¥ì´ ë°œìƒí•©ë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•´ ë””ìŠ¤í¬ I/Oë¥¼ ë¶„ì‚°ì‹œí‚¤ë©´, ë””ìŠ¤í¬ì˜ ë³‘ëª© í˜„ìƒì„ ì¤„ì´ê³  ì „ì²´ì ì¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    3. **ë³‘ë ¬ ì²˜ë¦¬**:
        - ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì—¬ëŸ¬ ë°°ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë©€í‹°ì½”ì–´ í”„ë¡œì„¸ì„œì˜ ì„±ëŠ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ë° ìœ ë¦¬í•©ë‹ˆë‹¤.
    
    ### **ë°ì´í„° ë°°ì¹˜ë€ ë¬´ì—‡ì¸ê°€ìš”?**
    
    ë°ì´í„° ë°°ì¹˜ëŠ” ì»´í“¨í„°ê°€ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ì‘ì€ ê·¸ë£¹ì„ ë§í•©ë‹ˆë‹¤. ëª¨ë“  ë°ì´í„°ë¥¼ í•œêº¼ë²ˆì— ì²˜ë¦¬í•˜ì§€ ì•Šê³ , ì—¬ëŸ¬ ë²ˆì— ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
    
    ### **ë¹„ìœ ë¡œ ì„¤ëª…í•˜ê¸°**
    
    ìƒê°í•´ë³´ì„¸ìš”. ì—¬ëŸ¬ë¶„ì´ ë§ì€ ì–‘ì˜ ì±…ì„ ì •ë¦¬í•´ì•¼ í•œë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. ë§Œì•½ ëª¨ë“  ì±…ì„ í•œêº¼ë²ˆì— ë“¤ë ¤ê³  í•œë‹¤ë©´, ë„ˆë¬´ ë¬´ê±°ì›Œì„œ í˜ë“¤ ê²ƒì…ë‹ˆë‹¤. ëŒ€ì‹ , ì‘ì€ ìƒìì— ëª‡ ê¶Œì”© ë‚˜ëˆ„ì–´ ë‹´ì•„ ì˜®ê¸´ë‹¤ë©´ ë” ì‰½ê³  íš¨ìœ¨ì ì¼ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° ë°°ì¹˜ë„ ì´ì™€ ë¹„ìŠ·í•œ ì›ë¦¬ì…ë‹ˆë‹¤. í•œêº¼ë²ˆì— ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹ , ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    
    ### **ë°ì´í„° ë°°ì¹˜ì˜ ì¥ì **
    
    1. **ë©”ëª¨ë¦¬ ì‚¬ìš© ê°ì†Œ**:
        - í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ì–‘ì„ ì¤„ì´ê¸° ë•Œë¬¸ì—, ë©”ëª¨ë¦¬ë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. **ì†ë„ í–¥ìƒ**:
        - ì‘ì€ ë°°ì¹˜ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë©´, ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¥¼ ë” ì˜ ê´€ë¦¬í•  ìˆ˜ ìˆì–´ ì „ì²´ ì²˜ë¦¬ ì†ë„ê°€ í–¥ìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    3. **ì—ëŸ¬ ë°©ì§€**:
        - ëª¨ë“  ë°ì´í„°ë¥¼ í•œêº¼ë²ˆì— ì²˜ë¦¬í•˜ë ¤ê³  í•˜ë©´ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•´ì ¸ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜ë¡œ ë‚˜ëˆ„ë©´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ### **ì˜ˆì‹œ**
    
    ì˜ˆë¥¼ ë“¤ì–´, 10,000ê°œì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²˜ë¦¬í•´ì•¼ í•œë‹¤ê³  ê°€ì •í•©ì‹œë‹¤. í•œêº¼ë²ˆì— ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´ ì»´í“¨í„°ì˜ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€ì‹ , í•œ ë²ˆì— 1,000ê°œì˜ íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë°°ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ 10,000ê°œì˜ íŒŒì¼ì„ 10ë²ˆì— ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ê²Œ ë©ë‹ˆë‹¤.
    

### ì‹¤íŒ¨ ì´ìœ 

ëŠê·¸ ì¹œêµ¬ gptê°€ ì¶”ì²œí•´ì¤€ ì½”ë“œë¥¼ ë©´ë°€íˆ ì‚´í´ë³´ë‹ˆ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì†Œí™” ì‹œí‚¤ê¸° ìœ„í•´ batchë¥¼ ì„¤ì •í•˜ê³  í•˜ë‚˜ì˜ batch íŠ¹ì„± ì¶”ê°€ê°€ ëë‚˜ë©´ ê·¸ batchë¥¼ ì´ˆê¸°í™” ì‹œì¼œ ë²„ë ¸ë‹¤(..?) ë„ëŒ€ì²´ ì™œ ê·¸ëŸ° ë¯¸ì¹œ ì§“ì„ í–ˆëŠ”ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ, ì•„ë¬´íŠ¼ ê²°êµ­ì—” ë˜‘ê°™ì´ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨í–ˆë‹¤

### ê°œì„  ì‚¬í•­

google colabì€ colabì—ì„œ gpuë¥¼ í• ë‹¹í•´ì¤€ë‹¤ê³  í•œë‹¤. ê·¸ë˜ì„œ ê°œë°œ í™˜ê²½ì„ visual studioì—ì„œ google colabìœ¼ë¡œ ë³€ê²½í•  ì˜ˆì •ì´ë‹¤.

ì¶”ê°€ë¡œ sample dataì™€ train data ë¶„ë¥˜í•˜ëŠ” ì½”ë“œë¥¼ ìƒˆë¡œ ì§œë³´ì

## 1-2-3-1

- code
    
    ```python
    import os
    import random
    import librosa
    import pickle
    import pandas as pd
    import numpy as np
    import librosa.display
    import matplotlib.pyplot as plt
    from scipy.fft import fft
    from sklearn.model_selection import train_test_split
    from tqdm.auto import tqdm
    
    from google.colab import drive
    drive.mount('/content/drive')
    
    base_path = '/content/drive/MyDrive/project/project1/sample'
    features_file_path = '/content/drive/MyDrive/project/project1/sample/features_by_key.pkl'
    output_dir = '/content/drive/MyDrive/project/project1/sample/stft_plots'
    
    #load files
    def load_audio_files(base_path, target_sr=22050, max_length=5.0):
      audio_data = {}
      file_index = 0
      for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
        files.sort()
        for file in tqdm(files, desc="Processing files", leave=False):
          if file.endswith((".wav", ".mp3")):
            parts=root.split(os.sep)
            effect=parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
            key = (effect, file_index)
            file_index += 1
            if key not in audio_data:
              audio_data[key]=[]
            file_path = os.path.join(root, file)
            data, sr = librosa.load(file_path, sr=None)
            #Amplitude Normalization
            data = librosa.util.normalize(data)
    
            audio_data[key].append((data, sr))
      return audio_data
    
    def load_sample_files(base_path, target_sr=22050, max_length=5.0):
        audio_data = {}
        file_index = 0
        for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
            effect_files = [file for file in files if file.endswith((".wav", ".mp3"))]
            if effect_files:
                file = random.choice(effect_files)  # ê° í´ë”ì—ì„œ ëœë¤ìœ¼ë¡œ 1ê°œì˜ íŒŒì¼ ì„ íƒ
                parts = root.split(os.sep)
                effect = parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
                key = (effect, file_index)
                file_index += 1
                if key not in audio_data:
                    audio_data[key] = []
                file_path = os.path.join(root, file)
                data, sr = librosa.load(file_path, sr=None)
                # Amplitude Normalization
                data = librosa.util.normalize(data)
                audio_data[key].append((data, sr))
        return audio_data
    
    #STFT funtion
    def ext_STFT_features(data, sr, n_fft=2048, hop_length=512):
      stft_vals = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)
       return np.abs(stft_vals)
    
    # preparing data
    def data(audio_files, batch_size=100):
      features_list=[]
      labels_list=[]
      effect_mapping={
            'BluesDriver': 'drive',
            'Chorus': 'chorus',
            'Clean': 'clean',
            'Digital Delay': 'delay',
            'Distortion': 'drive',
            'FeedbackDelay': 'delay',
            'Flanger': 'flanger',
            'Hall Reverb': 'reverb',
            'NoFX': 'clean',
            'Overdrive': 'drive',
            'Phaser': 'phaser',
            'Plate Reverb': 'reverb',
            'RAT': 'drive',
            'SlapbackDelay': 'delay',
            'Spring Reverb': 'reverb',
            'Sweep Echo': 'delay',
            'TapeEcho': 'delay',
            'Tremolo': 'tremolo',
            'TubeScreamer': 'drive',
            'Vibrato': 'vibrato'
      }
    
      label_mapping={
          'clean' : 0,
          'drive' : 1,
          'reverb' : 2,
          'delay' : 3,
          'chorus' : 4,
          'phaser' : 5,
          'flanger' : 6,
          'tremolo' : 7,
          'vibrato' : 8
      }
    
      keys = list(audio_files.keys())
      for start in tqdm(range(0, len(keys), batch_size), desc='Preparing data in batches'):
        batch_keys = keys[start:start + batch_size]
        for key in batch_keys:
          effect_type = key[0]
          category = effect_mapping.get(effect_type, None)
          if category in label_mapping:
            for data, sr in audio_files[key]:
              fft_features = ext_STFT_features(data, sr)
              features_list.append((fft_features,  sr, effect_type))
              labels_list.append(label_mapping[category])
      return features_list, labels_list
    
    #plot spectogram graph
    def plot_spectogram(feature, sr, title):
      plt.figure(figsize=(10,4))
      librosa.display.specshow(librosa.amplitude_to_db(feature, ref=np.max), sr=sr, hop_length=512, x_axis='time', y_axis='log')
      plt.colorbar(format='%+2.0f dB')
      plt.title(title)
      plt.tight_layout()
      plt.show()
    
    audio_files = load_sample_files(base_path)
    features, labels = data(audio_files)
    
    for feature, sr, effect_type in features:
        title = f"Spectogram of {effect_type}"
        plot_spectogram(feature, sr, title)
    ```
    
- result
    
    ![Unknown-3.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-3.png)
    
    ![Unknown-4.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-4.png)
    
    ![Unknown-5.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-5.png)
    
    ![Unknown-6.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-6.png)
    
    ![Unknown-7.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-7.png)
    
    ![Unknown-8.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-8.png)
    
    ![Unknown-9.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-9.png)
    
    ![Unknown-10.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-10.png)
    
    ![Unknown-11.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-11.png)
    
    ![Unknown-12.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-12.png)
    
    ![Unknown-13.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-13.png)
    
    ![Unknown-14.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-14.png)
    
    ![Unknown-15.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-15.png)
    
    ![Unknown-16.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-16.png)
    
    ![Unknown-17.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-17.png)
    
    ![Unknown-18.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-18.png)
    
    ![Unknown-19.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-19.png)
    
    ![Unknown-20.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-20.png)
    
    ![Unknown-21.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-21.png)
    
    ![Unknown-22.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-22.png)
    
    ![Unknown-23.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-23.png)
    
    ![Unknown-24.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-24.png)
    
    ![Unknown-25.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-25.png)
    
    ![Unknown-26.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-26.png)
    
    ![Unknown-27.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-27.png)
    
    ![Unknown-28.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-28.png)
    
    ![Unknown-29.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-29.png)
    
    ![Unknown-30.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-30.png)
    
    ![Unknown-31.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-31.png)
    
    ![Unknown-32.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-32.png)
    
    ![Unknown-33.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-33.png)
    
    ![Unknown-34.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-34.png)
    
    ![Unknown-35.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-35.png)
    
    ![Unknown-36.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-36.png)
    
    ![Unknown-37.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-37.png)
    
    ![Unknown-38.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-38.png)
    
    ![Unknown-39.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-39.png)
    
    ![Unknown-40.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-40.png)
    
    ![Unknown-41.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-41.png)
    
    ![Unknown-42.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-42.png)
    
    ![Unknown-43.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-43.png)
    
    ![Unknown-44.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-44.png)
    
    ![Unknown-45.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-45.png)
    
    ![Unknown-46.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-46.png)
    
    ![Unknown-47.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-47.png)
    
    ![Unknown-48.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-48.png)
    
    ![Unknown-49.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-49.png)
    
    ![Unknown-50.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-50.png)
    
    ![Unknown-51.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-51.png)
    
    ![Unknown-52.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-52.png)
    
    ![Unknown-53.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-53.png)
    
    ![Unknown-54.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-54.png)
    
    ![Unknown-55.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-55.png)
    
    ![Unknown-56.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-56.png)
    
    ![Unknown-57.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-57.png)
    
- ìˆ˜ì •ì‚¬í•­
    
    ### 1. ê°œë°œ í™˜ê²½
    
    visual studio â†’ google colab
    
    colabì´ ramì§€ì›ì„ í•´ì¤Œìœ¼ë¡œì¨ ì»´í“¨í„° ë©”ëª¨ë¦¬ìœ¼ë¡œë§Œ ëŒë¦´ ë•ŒëŠ” killëë˜ ë°ì´í„° ë¡œë”©ì´ ì´ì œëŠ” ê°€ëŠ¥í•´ì§€ê²Œ ë˜ì—ˆë‹¤.
    
    ë°ì´í„° íŒŒì¼ë„ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ë„£ì–´ì„œ ì—°ë™ì‹œí‚¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì»´í“¨í„° ìš©ëŸ‰ì€ ë” ì±™ê¸¸ ìˆ˜ ìˆëŠ” ê²ƒì€ ë¤
    
    ### 2. sample ì„ íƒ
    
    load_sample_files í•¨ìˆ˜ ë„ì…
    
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì²˜ìŒì—ëŠ” batchë¥¼ ì‚¬ìš©í•˜ëŠ” ë“±ì˜ ë°©ë²•ì„ ì„ íƒí–ˆìœ¼ë‚˜ ì†Œìš”ì‹œê°„ì€ ë¹„ìŠ·í–ˆìŒ ì–´ì§œí”¼ ì§€ê¸ˆì€ ë°ì´í„° featureê°€ ì˜ ë‚˜ì˜¤ëŠ”ì§€ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì€ê±°ë¼ ê° ì´í™í„°ë³„(í´ë”ë³„) ìƒ˜í”Œ ë°ì´í„° í•˜ë‚˜ë§Œ ì„ íƒí•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒˆë¡œ ì œì‘ â†’ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì„
    

ê²°ê³¼ë¥¼ ë³´ì•„í•˜ë‹ˆ ê° ì´í™í„°ë³„ íŠ¹ì„±ì´ ì˜ ë“œëŸ¬ë‚˜ìˆëŠ” ê²ƒ ê°™ë‹¤. ê·¸ë¦¬ê³  íŠ¹íˆ 1ë²ˆ dataê°€ í™•ì‹¤íˆ data ìˆ˜ëŠ” ì ì–´ë„ í€„ë¦¬í‹°ëŠ” 2ë²ˆë³´ë‹¤ í›¨ ì¢‹ì€ë“¯

data ì¤‘ì— ì•½ 0.5ì´ˆ delayëœ dataë“¤ì´ ë§ì€ë° ì´ë¥¼ ì–´ë–»ê²Œ í•´ê²°í•´ì•¼í• ì§€ ê³ ë¯¼í•´ë´ì•¼ê² ë‹¤

## 1-2-3-2

- code
    
    ```python
    import os
    import random
    import librosa
    import pickle
    import pandas as pd
    import numpy as np
    import librosa.display
    import matplotlib.pyplot as plt
    from scipy.fft import fft
    from sklearn.model_selection import train_test_split
    from tqdm.auto import tqdm
    
    from google.colab import drive
    drive.mount('/content/drive')
    
    base_path = '/content/drive/MyDrive/project/project1/sample'
    features_file_path = '/content/drive/MyDrive/project/project1/sample/features_by_key.pkl'
    output_dir = '/content/drive/MyDrive/project/project1/sample/stft_plots'
    #remove silent segment
    def remove_silent(data, sr, db_threshold=-20, freq_threshold=4096, hop_length=512, n_fft=2048):
    
      stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)
      freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)
    
      freq_idx = np.where(freqs >= freq_threshold)[0][0]
    
      magnitude = np.abs(stft[freq_idx:, :])
      db_values = librosa.amplitude_to_db(magnitude, ref=np.max)
    
      start_frame = np.argmax(np.any(db_values > db_threshold, axis=0))
    
      if start_frame == 0 and np.all(db_values[:,0]<=db_threshold):
        return stft
    
      return stft[:, start_frame:]
    
    #load files
    def load_audio_files(base_path, target_sr=22050, max_length=5.0):
      audio_data = {}
      file_index = 0
      for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
        files.sort()
        for file in tqdm(files, desc="Processing files", leave=False):
          if file.endswith((".wav", ".mp3")):
            parts=root.split(os.sep)
            effect=parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
            key = (effect, file_index)
            file_index += 1
            if key not in audio_data:
              audio_data[key]=[]
            file_path = os.path.join(root, file)
            data, sr = librosa.load(file_path, sr=None)
            #Trim leading and trailing silence
            data, _ = librosa.effects.trim(data)
    
            #Remove silent segment
            data = remove_silent(data, sr, db_threshold=-20, freq_threshold=4096)
            
            # Amplitude Normalization
            data = librosa.util.normalize(data)
    
            audio_data[key].append((data, sr))
      return audio_data
    
    def load_sample_files(base_path, target_sr=22050, max_length=5.0):
        audio_data = {}
        file_index = 0
        for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
            effect_files = [file for file in files if file.endswith((".wav", ".mp3"))]
            if effect_files:
                file = random.choice(effect_files)  # ê° í´ë”ì—ì„œ ëœë¤ìœ¼ë¡œ 1ê°œì˜ íŒŒì¼ ì„ íƒ
                parts = root.split(os.sep)
                effect = parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
                key = (effect, file_index)
                file_index += 1
                if key not in audio_data:
                    audio_data[key] = []
                file_path = os.path.join(root, file)
                data, sr = librosa.load(file_path, sr=None)
                #Trim leading and trailing silence
                data, _ = librosa.effects.trim(data)
    
                #Remove silent segment
                data = remove_silent(data, sr, db_threshold=-20, freq_threshold=4096)
    
                # Amplitude Normalization
                data = librosa.util.normalize(data
                
                audio_data[key].append((data, sr))
        return audio_data
    
    #STFT funtion
    '''
    def ext_STFT_features(data, sr, n_fft=2048, hop_length=512):
      stft_vals = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)
       return np.abs(stft_vals)
    '''
    
    # preparing data
    def data(audio_files, batch_size=100):
      features_list=[]
      labels_list=[]
      effect_mapping={
            'BluesDriver': 'drive',
            'Chorus': 'chorus',
            'Clean': 'clean',
            'Digital Delay': 'delay',
            'Distortion': 'drive',
            'FeedbackDelay': 'delay',
            'Flanger': 'flanger',
            'Hall Reverb': 'reverb',
            'NoFX': 'clean',
            'Overdrive': 'drive',
            'Phaser': 'phaser',
            'Plate Reverb': 'reverb',
            'RAT': 'drive',
            'SlapbackDelay': 'delay',
            'Spring Reverb': 'reverb',
            'Sweep Echo': 'delay',
            'TapeEcho': 'delay',
            'Tremolo': 'tremolo',
            'TubeScreamer': 'drive',
            'Vibrato': 'vibrato'
      }
    
      label_mapping={
          'clean' : 0,
          'drive' : 1,
          'reverb' : 2,
          'delay' : 3,
          'chorus' : 4,
          'phaser' : 5,
          'flanger' : 6,
          'tremolo' : 7,
          'vibrato' : 8
      }
    
      keys = list(audio_files.keys())
      for start in tqdm(range(0, len(keys), batch_size), desc='Preparing data in batches'):
        batch_keys = keys[start:start + batch_size]
        for key in batch_keys:
          effect_type = key[0]
          category = effect_mapping.get(effect_type, None)
          if category in label_mapping:
            for stft, sr in audio_files[key]:
              features_list.append((stft,  sr, effect_type))
              labels_list.append(label_mapping[category])
      return features_list, labels_list
    
    #plot spectogram graph
    def plot_spectogram(feature, sr, title):
      plt.figure(figsize=(10,4))
      librosa.display.specshow(librosa.amplitude_to_db(feature, ref=np.max), sr=sr, hop_length=512, x_axis='time', y_axis='log')
      plt.colorbar(format='%+2.0f dB')
      plt.title(title)
      plt.tight_layout()
      plt.show()
    
    audio_files = load_sample_files(base_path)
    features, labels = data(audio_files)
    
    for feature, sr, effect_type in features:
        title = f"Spectogram of {effect_type}"
        plot_spectogram(feature, sr, title)
    ```
    
- result
    
    ![Unknown-59.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-59.png)
    
    ![Unknown-60.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-60.png)
    
    ![Unknown-61.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-61.png)
    
    ![Unknown-62.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-62.png)
    
    ![Unknown-63.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-63.png)
    
    ![Unknown-64.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-64.png)
    
    ![Unknown-65.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-65.png)
    
    ![Unknown-66.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-66.png)
    
    ![Unknown-67.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-67.png)
    
    ![Unknown-68.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-68.png)
    
    ![Unknown-69.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-69.png)
    
    ![Unknown-70.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-70.png)
    
    ![Unknown-71.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-71.png)
    
    ![Unknown-72.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-72.png)
    
    ![Unknown-73.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-73.png)
    
    ![Unknown-74.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-74.png)
    
    ![Unknown-75.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-75.png)
    
    ![Unknown-76.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-76.png)
    
    ![Unknown-77.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-77.png)
    
    ![Unknown-78.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-78.png)
    
    ![Unknown-79.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-79.png)
    
    ![Unknown-80.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-80.png)
    
    ![Unknown-81.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-81.png)
    
    ![Unknown-82.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-82.png)
    
    ![Unknown-83.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-83.png)
    
    ![Unknown-84.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-84.png)
    
    ![Unknown-85.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-85.png)
    
    ![Unknown-86.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-86.png)
    
    ![Unknown-87.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-87.png)
    
    ![Unknown-88.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-88.png)
    
    ![Unknown-89.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-89.png)
    
    ![Unknown-90.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-90.png)
    
    ![Unknown-91.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-91.png)
    
    ![Unknown-92.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-92.png)
    
    ![Unknown-93.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-93.png)
    
    ![Unknown-94.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-94.png)
    
    ![Unknown-95.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-95.png)
    
    ![Unknown-96.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-96.png)
    
    ![Unknown-97.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-97.png)
    
    ![Unknown-98.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-98.png)
    
    ![Unknown-99.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-99.png)
    
    ![Unknown-100.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-100.png)
    
    ![Unknown-101.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-101.png)
    
    ![Unknown-102.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-102.png)
    
    ![Unknown-103.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-103.png)
    
    ![Unknown-104.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-104.png)
    
    ![Unknown-105.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-105.png)
    
    ![Unknown-106.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-106.png)
    
    ![Unknown-107.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-107.png)
    
    ![Unknown-108.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-108.png)
    
    ![Unknown-109.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-109.png)
    
    ![Unknown-110.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-110.png)
    
    ![Unknown-111.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-111.png)
    
    ![Unknown-112.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-112.png)
    
    ![Unknown-113.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Unknown-113.png)
    
- ìˆ˜ì •ì‚¬í•­
    
    ### 1. remove_silent í•¨ìˆ˜ ì¶”ê°€
    
    remove silent í•¨ìˆ˜ì—ì„œ frequence, magnitude thresholdë¥¼ ì„¤ì •í•˜ê³ , ì²˜ìŒìœ¼ë¡œ thresholdê°€ ë„˜ëŠ” êµ¬ê°„ì´ ë‚˜íƒ€ë‚˜ë©´ ê·¸ êµ¬ê°„ì„ sampleì˜ ì‹œì‘ìœ¼ë¡œ ë‘ì—ˆë‹¤
    
    ë˜í•œ freq, magë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œëŠ” librosa.stftë¥¼ ë˜ ëŒë ¤ì•¼í•˜ê¸° ë•Œë¬¸ì— ext_stftë¥¼ ì œê±°í–ˆë‹¤
    
    ### 2. load_files í•¨ìˆ˜ ìˆ˜ì •
    
    librosaì—ì„œ ì œê³µí•˜ëŠ” trimì„ í•œë²ˆ ê±°ì¹˜ê³ , ê·¸ë‹¤ìŒ remove_silentí•¨ìˆ˜ë¥¼ ê±°ì¹œ í›„ì— Amplifyë¥¼ ë‚˜ì¤‘ì— ê±°ì³¤ë‹¤. Amplifyë¥¼ ë¨¼ì € ê±°ì¹˜ë‹ˆ ê³„ì† sample 2ê°€ freq, dbê°€ ì¦ê°€í•˜ê²Œ ë˜ë©´ì„œ trimmingì´ íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë˜ì§€ ì•Šì•˜ë˜ê²ƒ ê°™ë‹¤
    

ì—¬ëŸ¬ë²ˆì˜ ì‹œí–‰ì°©ì˜¤ ëì— ëª¨ë“  ë°ì´í„° ì•ì˜ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œ ë°ì´í„°ë¥¼ ë§Œë“¤ì—ˆë‹¤..! ì˜¤ëœ ì‹œê°„ ëì— ë“œë””ì–´ ì¸ê³µì§€ëŠ¥ í•™ìŠµì´ ê°€ëŠ¥í•˜ê²Œ ëœ ê²ƒì´ë‹¤ ì´ì œ ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ ê°ì¢… í•™ìŠµ ë°ì´í„°ë“¤ì„ ì¶”ì¶œí•˜ê³ , cnní•™ìŠµì— ë“¤ì–´ê°€ë³´ë„ë¡ í•˜ì

â†’ stft ë°ì´í„°ëŠ” 2ì°¨ì› matrixì´ê¸° ë•Œë¬¸ì— 1ì°¨ì› matrixì— ëŒ€í•œ í•™ìŠµë§Œ ê°€ëŠ¥í•œ deep learningìœ¼ë¡œëŠ” í•™ìŠµì´ ë¶ˆê°€ / ë°”ë¡œ cnní•™ìŠµìœ¼ë¡œ ë“¤ì–´ê°€ì ë“¤ì–´ê°€ê¸°ì— ì•ì„œ cnnì— ëŒ€í•´ ê³µë¶€ë¥¼ ì¡°ê¸ˆ í•´ë³´ë„ë¡ í•˜ì

[ë”¥ëŸ¬ë‹ CNN, ê°œë…ë§Œ ì´í•´í•˜ê¸°](https://youtu.be/9Cu2UfNO-gw?si=EODUu130rvfiPKTf)

[ë”¥ëŸ¬ë‹ CNN, ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§, í•©ì„±ê³± ì‹ ê²½ë§ ê°œë… ì •ë¦¬](https://youtu.be/ZZKnBpd1lR4?si=2OuPxSicM87ZnkgP)

[Neural networks](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=4e9PDMYtE7XEzr8N)

[íŒŒì´í† ì¹˜ PyTorch](https://youtube.com/playlist?list=PL7ZVZgsnLwEEIC4-KQIchiPda_EjxX61r&si=jrawP7GMW8IrTUWy)

## 1-3-1

- code
    
    ```python
    import os
    import random
    import librosa
    import pickle
    import pandas as pd
    import numpy as np
    import librosa.display
    import matplotlib.pyplot as plt
    from scipy.fft import fft
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, accuracy_score
    from sklearn.preprocessing import StandardScaler
    from tqdm.auto import tqdm
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    
    from google.colab import drive
    drive.mount('/content/drive')
    
    base_path = '/content/drive/MyDrive/project/project1/sample'
    features_file_path = '/content/drive/MyDrive/project/project1/sample/features_by_key.pkl'
    output_dir = '/content/drive/MyDrive/project/project1/sample/stft_plots'
    
    #remove silent segment
    def remove_silent(data, sr, db_threshold=-20, freq_threshold=4096, hop_length=512, n_fft=2048):
    
      stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)
      freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)
    
      freq_idx = np.where(freqs >= freq_threshold)[0][0]
    
      magnitude = np.abs(stft[freq_idx:, :])
      db_values = librosa.amplitude_to_db(magnitude, ref=np.max)
    
      start_frame = np.argmax(np.any(db_values > db_threshold, axis=0))
    
      if start_frame == 0 and np.all(db_values[:,0]<=db_threshold):
        return stft
    
      return stft[:, start_frame:]
    
    #load files
    def load_audio_files(base_path, target_sr=22050, max_length=5.0):
      audio_data = {}
      file_index = 0
      for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
        files.sort()
        for file in tqdm(files, desc="Processing files", leave=False):
          if file.endswith((".wav", ".mp3")):
            parts=root.split(os.sep)
            effect=parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
            key = (effect, file_index)
            file_index += 1
            if key not in audio_data:
              audio_data[key]=[]
            file_path = os.path.join(root, file)
            data, sr = librosa.load(file_path, sr=None)
            #Trim leading and trailing silence
            data, _ = librosa.effects.trim(data)
    
            #Remove silent segment
            data = remove_silent(data, sr, db_threshold=-20, freq_threshold=4096)
    
            # Amplitude Normalization
            data = librosa.util.normalize(data)
    
            audio_data[key].append((data, sr))
      return audio_data
    
    def load_sample_files(base_path, target_sr=22050, max_length=5.0):
        audio_data = {}
        file_index = 0
        for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
            effect_files = [file for file in files if file.endswith((".wav", ".mp3"))]
            if effect_files:
                file = random.choice(effect_files)  # ê° í´ë”ì—ì„œ ëœë¤ìœ¼ë¡œ 1ê°œì˜ íŒŒì¼ ì„ íƒ
                parts = root.split(os.sep)
                effect = parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
                key = (effect, file_index)
                file_index += 1
                if key not in audio_data:
                    audio_data[key] = []
                file_path = os.path.join(root, file)
                data, sr = librosa.load(file_path, sr=None)
                #Trim leading and trailing silence
                data, _ = librosa.effects.trim(data)
    
                #Remove silent segment
                data = remove_silent(data, sr, db_threshold=-20, freq_threshold=4096)
    
                # Amplitude Normalization
                data = librosa.util.normalize(data)
    
                audio_data[key].append((data, sr))
        return audio_data
    
    #STFT funtion
    '''
    def ext_STFT_features(data, sr, n_fft=2048, hop_length=512):
      stft_vals = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)
      return np.abs(stft_vals)
    '''
    
    # preparing data
    def data(audio_files, batch_size=100):
      features_list=[]
      labels_list=[]
      effect_mapping={
            'BluesDriver': 'drive',
            'Chorus': 'chorus',
            'Clean': 'clean',
            'Digital-Delay': 'delay',
            'Distortion': 'drive',
            'FeedbackDelay': 'delay',
            'Flanger': 'flanger',
            'Hall-Reverb': 'reverb',
            'NoFX': 'clean',
            'Overdrive': 'drive',
            'Phaser': 'phaser',
            'Plate-Reverb': 'reverb',
            'RAT': 'drive',
            'SlapbackDelay': 'delay',
            'Spring-Reverb': 'reverb',
            'Sweep-Echo': 'delay',
            'TapeEcho': 'delay',
            'Tremolo': 'tremolo',
            'TubeScreamer': 'drive',
            'Vibrato': 'vibrato'
      }
    
      label_mapping={
          'clean' : 0,
          'drive' : 1,
          'reverb' : 2,
          'delay' : 3,
          'chorus' : 4,
          'phaser' : 5,
          'flanger' : 6,
          'tremolo' : 7,
          'vibrato' : 8
      }
    
      keys = list(audio_files.keys())
      for start in tqdm(range(0, len(keys), batch_size), desc='Preparing data in batches'):
        batch_keys = keys[start:start + batch_size]
        for key in batch_keys:
          effect_type = key[0]
          category = effect_mapping.get(effect_type, None)
          if category in label_mapping:
            for stft in audio_files[key]:
              features_list.append(stft)
              labels_list.append(label_mapping[category])
      return features_list, labels_list, label_mapping
    
    #plot spectogram graph
    def plot_spectogram(feature, sr, title):
      plt.figure(figsize=(10,4))
      librosa.display.specshow(librosa.amplitude_to_db(feature, ref=np.max), sr=sr, hop_length=512, x_axis='time', y_axis='log')
      plt.colorbar(format='%+2.0f dB')
      plt.title(title)
      plt.tight_layout()
      plt.show()
    
    #CNN_dataset
    class AudioDataset(Dataset):
      def __init__(self, features, labels):
        self.features = [f[0] for f in features]
        self.labels = labels
    
        self.features = [self._pad_feature(f) for f in self.features]
    
      def __len__(self):
        return len(self.features)
    
      def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]
        feature = np.expand_dims(feature, axis=0)
        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.long)
    
      def _pad_feature(self, feature):
        max_shape = (1025,470)
        padded_feature = np.zeros(max_shape)
        padded_feature[:feature.shape[0], :feature.shape[1]]=feature
        return padded_feature
    
    #CNN_model
    class AudioCNN(nn.Module):
      def __init__(self, num_classes):
        super(AudioCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16,32, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.fc1 = nn.Linear(64*128*58, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 =nn.Linear(128, num_classes)
    
      def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0),-1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
    
    audio_files = load_audio_files(base_path)
    
    features, labels, label_mapping  = data(audio_files)
    
    #for feature, sr, effect_type in features:
    #    title = f"Spectogram of {effect_type}"
    #    plot_spectogram(feature, sr, title)
    
    #generate dataset / dataloader
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
    train_dataset = AudioDataset(X_train, y_train)
    test_dataset = AudioDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')
    model = AudioCNN(num_classes=len(label_mapping)).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    num_epochs = 10
    
    for epoch in tqdm(range(num_epochs)):
      model.train()
      running_loss = 0.0
      for inputs, labels in tqdm(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
      print(f'Epoch : {epoch+1}/{num_epochs}, Loss : {running_loss/len(train_loader)}')
    
      model.eval()
      correct = 0
      total = 0
      with torch.no_grad():
        for inputs, labels in test_loader:
          inputs, labels = inputs.to(device), labels.to(device)
          outputs = model(inputs)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
    
      print(f'Accuracy: {100*correct/total}%')
    
    ```
    
- result
    
    ![Untitled.png](PROJECT%201%203b3a6a3def584c049c5a32639cace89d/Untitled.png)
    
- ìˆ˜ì •ì‚¬í•­
    
    ### 1. data í•¨ìˆ˜ ìˆ˜ì •
    
    features ë°ì´í„° íƒ€ì…ì´ íŠœí”Œ (sr, features, effector types)ë¡œ ì €ì¥ë˜ì–´ ìˆì–´ì„œ matrix ê³„ì‚°ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë“¤ì´ ë¨¹íˆì§€ ì•Šì•˜ë‹¤. ê·¸ë¦¬ê³  ì•„ë§ˆ labelsì™€ ì°¨ì›ì´ ë§ì§€ ì•Šì•„(labelsëŠ” 1ì°¨ì›ì´ì—ˆìŒ) ëª¨ë¸ í›ˆë ¨ì—ì„œ ì˜¤ë¥˜ê°€ ë‚¬ë˜ ê²ƒ ê°™ë‹¤. ì´ë¥¼ ìœ„í•´ dataí•¨ìˆ˜ì—ì„œ features_listì— featureë§Œ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •í•˜ì˜€ë‹¤. ì¶”ê°€ë¡œ label_mappingì„ Data í•¨ìˆ˜ ì•ˆì—ì„œ ë¡œì»¬ë¡œ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— ì¶”í›„ ë’¤ì— í›ˆë ¨ì„ ìœ„í•´ label_mappingì„ ë¶ˆëŸ¬ì™€ì•¼ í•˜ëŠ”ë° ìê¾¸ label_mappingì„ ì¶œë ¥í•˜ë©´ ë¹ˆ ì…‹ì´ ì¶œë ¥ë˜ì–´ ë­ë•Œë§¤ ê·¸ëŸ°ê±´ ì¤„ ëª°ëëŠ”ë°, label_mappingë„ dataí•¨ìˆ˜ì˜ return ê°’ìœ¼ë¡œ ì„¤ì •í–ˆë”ë‹ˆ ì•„ì£¼ ì˜ ë˜ì—ˆë‹¤. ìƒˆë¡œìš´ê±¸ ë°°ì› ë‹¤â€¦
    

92.5%ë¼ëŠ” ì •í™•ë„ê°€ ë‚˜ì™”ë‹¤ ì´ì „ì— ë¹„í•´ì„œ í™•ì‹¤íˆ ì¢‹ì•„ì§€ê¸´ í–ˆì§€ë§Œ ì•„ì§ í•œì°¸ ë¯¸ë‹¬ì´ë‹¤. ì—¬ëŸ¬ê°€ì§€ í™•ì¸ì‚¬í•­ë“¤ì„ í™•ì¸í•´ë³´ê³  ì •í™•ë„ë¥¼ ë” ë†’ì—¬ë³´ì ì¼ë‹¨ ì¬í˜„ìœ¨ì´ë‘ ì •ë°€ë„ë¥¼ ì¸¡ì •í•´ë³´ê³ , ê°ì¢… parameterë“¤ì„ ì¡°ì •í•˜ë©´ì„œ ì •í™•ë„ë¥¼ ë†’ì—¬ë³´ë„ë¡ í•˜ì

## 1-3-2

- code
    
    ```python
    import os
    import random
    import librosa
    import pickle
    import pandas as pd
    import numpy as np
    import librosa.display
    import matplotlib.pyplot as plt
    from scipy.fft import fft
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, accuracy_score
    from sklearn.preprocessing import StandardScaler
    from tqdm.auto import tqdm
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    
    from google.colab import drive
    drive.mount('/content/drive')
    
    base_path = '/content/drive/MyDrive/project/project1/sample'
    features_file_path = '/content/drive/MyDrive/project/project1/sample/features_by_key.pkl'
    output_dir = '/content/drive/MyDrive/project/project1/sample/stft_plots'
    
    #remove silent segment
    def remove_silent(data, sr, db_threshold=-20, freq_threshold=4096, hop_length=512, n_fft=2048):
    
      stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)
      freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)
    
      freq_idx = np.where(freqs >= freq_threshold)[0][0]
    
      magnitude = np.abs(stft[freq_idx:, :])
      db_values = librosa.amplitude_to_db(magnitude, ref=np.max)
    
      start_frame = np.argmax(np.any(db_values > db_threshold, axis=0))
    
      if start_frame == 0 and np.all(db_values[:,0]<=db_threshold):
        return stft
    
      return stft[:, start_frame:]
      
      #load files
    def load_audio_files(base_path, target_sr=22050, max_length=5.0):
      audio_data = {}
      file_index = 0
      for root, dirs, files in tqdm(os.walk(base_path), desc='Walking through directories'):
        files.sort()
        for file in tqdm(files, desc="Processing files", leave=False):
          if file.endswith((".wav", ".mp3")):
            parts=root.split(os.sep)
            effect=parts[-3] if 'samples' not in parts[-2].lower() else parts[-1]
            key = (effect, file_index)
            file_index += 1
            if key not in audio_data:
              audio_data[key]=[]
            file_path = os.path.join(root, file)
            data, sr = librosa.load(file_path, sr=None)
            #Trim leading and trailing silence
            data, _ = librosa.effects.trim(data)
    
            #Remove silent segment
            data = remove_silent(data, sr, db_threshold=-20, freq_threshold=4096)
    
            # Amplitude Normalization
            data = librosa.util.normalize(data)
    
            audio_data[key].append((data, sr))
      return audio_data
      
      # preparing data
    def data(audio_files, batch_size=100):
      features_list=[]
      labels_list=[]
      effect_mapping={
            'BluesDriver': 'drive',
            'Chorus': 'chorus',
            'Clean': 'clean',
            'Digital-Delay': 'delay',
            'Distortion': 'drive',
            'FeedbackDelay': 'delay',
            'Flanger': 'flanger',
            'Hall-Reverb': 'reverb',
            'NoFX': 'clean',
            'Overdrive': 'drive',
            'Phaser': 'phaser',
            'Plate-Reverb': 'reverb',
            'RAT': 'drive',
            'SlapbackDelay': 'delay',
            'Spring-Reverb': 'reverb',
            'Sweep-Echo': 'delay',
            'TapeEcho': 'delay',
            'Tremolo': 'tremolo',
            'TubeScreamer': 'drive',
            'Vibrato': 'vibrato'
      }
    
      label_mapping={
          'clean' : 0,
          'drive' : 1,
          'reverb' : 2,
          'delay' : 3,
          'chorus' : 4,
          'phaser' : 5,
          'flanger' : 6,
          'tremolo' : 7,
          'vibrato' : 8
      }
    
      keys = list(audio_files.keys())
      for start in tqdm(range(0, len(keys), batch_size), desc='Preparing data in batches'):
        batch_keys = keys[start:start + batch_size]
        for key in batch_keys:
          effect_type = key[0]
          category = effect_mapping.get(effect_type, None)
          if category in label_mapping:
            for stft in audio_files[key]:
              features_list.append(stft)
              labels_list.append(label_mapping[category])
      return features_list, labels_list, label_mapping
      
    audio_files = load_audio_files(base_path)
    features, labels, label_mapping  = data(audio_files)
      
      #CNN_dataset
    class AudioDataset(Dataset):
      def __init__(self, features, labels):
        self.features = [f[0] for f in features]
        self.labels = labels
    
        self.features = [self._pad_feature(f) for f in self.features]
    
      def __len__(self):
        return len(self.features)
    
      def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]
        feature = np.expand_dims(feature, axis=0)
        return torch.tensor(feature, dtype=torch.float32), torch.tensor(label, dtype=torch.long)
    
      def _pad_feature(self, feature):
        max_shape = (1025,470)
        padded_feature = np.zeros(max_shape)
        padded_feature[:feature.shape[0], :feature.shape[1]]=feature
        return padded_feature
    
    #CNN_model
    class AudioCNN(nn.Module):
      def __init__(self, num_classes):
        super(AudioCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16,32, kernel_size=3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.fc1 = nn.Linear(64*128*58, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 =nn.Linear(128, num_classes)
    
      def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(x.size(0),-1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
        
    #generate dataset / dataloader
    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)
    train_dataset = AudioDataset(X_train, y_train)
    test_dataset = AudioDataset(X_test, y_test)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    device = torch.device('cuda'if torch.cuda.is_available() else 'cpu')
    model = AudioCNN(num_classes=len(label_mapping)).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    num_epochs = 10
    
    for epoch in tqdm(range(num_epochs)):
      model.train()
      running_loss = 0.0
      for inputs, labels in tqdm(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
      print(f'Epoch : {epoch+1}/{num_epochs}, Loss : {running_loss/len(train_loader)}')
    
      model.eval()
      correct = 0
      total = 0
      with torch.no_grad():
        for inputs, labels in test_loader:
          inputs, labels = inputs.to(device), labels.to(device)
          outputs = model(inputs)
          _, predicted = torch.max(outputs.data, 1)
          total += labels.size(0)
          correct += (predicted == labels).sum().item()
    
      print(f'Accuracy: {100*correct/total}%')
      
    from sklearn.metrics import precision_recall_fscore_support
    
    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
      for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data,1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())
    
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average = 'weighted')
    print(f"Precision: {precision: .4f}, Recall: {recall: .4f}, F1 Score: {f1: .4f}")
    ```
    
- result
- ìˆ˜ì •ì‚¬í•­

ì•„ì§ ëª» ëŒë ¤ë´„

GANì„ í•™ìŠµí•´ë³´ì

[[Pytorch] GAN êµ¬í˜„ ë° í•™ìŠµ](https://ddongwon.tistory.com/124)

[GANs vs. Diffusion Models: Putting AI to the test](https://aurorasolar.com/blog/putting-ai-to-the-test-generative-adversarial-networks-vs-diffusion-models/)

[[ë…¼ë¬¸ë¦¬ë·°] GANSynth: Adversarial Neural Audio Synthesis (ICLR19)](https://music-audio-ai.tistory.com/13)

## 2-1-1

- code
    
    ```python
    #ì´í•˜ ìƒëµ
      
    audio_files = load_audio_files(base_path)
    features, labels, label_mapping  = data(audio_files)
      
    #data preparing for GAN generate
    clean_files = []
    drive_files = []
    
    for key, stfts in audio_files.items():
      effect_type=key[0]
      if effect_type in ['Clean','NoFX']:
        clean_files.extend([stft for stft in stfts])
      elif effect_type in ['BluesDriver', 'Distortion', 'Overdrive', 'RAT', 'TubeScreamer']:
        drive_files.extend([stft for stft in stfts])
    
    paired_data = []
    for clean, effect in zip(clean_files, drive_files):
      clean_stft, sr = clean
      effect_stft, _ = effect
    #  if clean_sr != effect_sr:
    #    raise ValueError('Sample rates do not match!')
      paired_data.append((clean_stft, effect_stft))
    
    class GANAudioDataset(Dataset):
      def __init__(self, paired_data):
        self.clean_stfts = [data[0] for data in paired_data]
        self.effect_stfts = [data[1] for data in paired_data]
        self.max_shape = (1025, max([stft.shape[1] for stft in self.clean_stfts+self.effect_stfts]))
    
      def __len__(self):
        return len(self.clean_stfts)
    
      def __getitem__(self, idx):
        clean_stft = self._pad_feature(self.clean_stfts[idx])
        effect_stft = self._pad_feature(self.effect_stfts[idx])
        clean_stft = np.stack((clean_stft.real, clean_stft.imag), axis=0)  
        effect_stft = np.stack((effect_stft.real, effect_stft.imag), axis=0)
        return torch.tensor(clean_stft, dtype=torch.float32), torch.tensor(effect_stft, dtype=torch.float32)
    
      def _pad_feature(self, feature):
        padded_feature = np.zeros(self.max_shape, dtype=np.complex64)
        padded_feature[:feature.shape[0], :feature.shape[1]]=feature
        return padded_feature
    
    dataset = GANAudioDataset(paired_data)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    class Generator(nn.Module):
      def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )
    
      def forward(self, x):
        return self.main(x)
    
    class Discriminator(nn.Module):
      def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(2,64,kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64,128,kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256,1, kernel_size=4, stride=1, padding=0),
            nn.Sigmoid()
        )
    
      def forward(self, x):
        return self.main(x)
        
    #training model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    generator = Generator().to(device)
    discriminator = Discriminator().to(device)
    
    criterion = nn.BCELoss()
    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5,0.999))
    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5,0.999))
    
    num_epochs = 50
    
    for epoch in tqdm(range(num_epochs), desc=f'{epoch+1} time updating'):
      for clean_stft, effect_stft in tqdm(dataloader):
        clean_stft, effect_stft = clean_stft.to(device), effect_stft.to(device)
        
        # Train Discriminator
        discriminator.zero_grad()
        real_outputs = discriminator(effect_stft)
        real_labels = torch.ones_like(real_outputs)
        real_loss = criterion(real_outputs, real_labels)
        real_loss.backward()
    
        fake_stft = generator(clean_stft)
        fake_outputs = discriminator(fake_stft.detach())
        fake_labels = torch.zeros_like(fake_outputs)
        fake_loss = criterion(fake_outputs, fake_labels)
        fake_loss.backward()
        optimizer_d.step()
    
        # Train Generator
        generator.zero_grad()
        fake_outputs = discriminator(fake_stft)
        g_loss = criterion(fake_outputs, real_labels)
        g_loss.backward()
        optimizer_g.step()
    
      print(f'Epoch [{epoch+1}/{num_epochs}], d_loss : {(real_loss.item()+fake_loss.item())/2}, g_loss : {g_loss.item()}')
    
    #Save the trained models
    generator_save_path = '/content/drive/MyDrive/project/project1/models/generator.pth'
    discriminator_save_path = '/content/drive/MyDrive/project/project1/models/discriminator.pth'
    
    # Save the trained models to Google Drive
    torch.save(generator.state_dict(), generator_save_path)
    torch.save(discriminator.state_dict(), discriminator_save_path)
    
    print(f"Generator model saved to {generator_save_path}")
    print(f"Discriminator model saved to {discriminator_save_path}")
    
    def load_generator(model_path, device):
        model = Generator().to(device)
        model.load_state_dict(torch.load(model_path))
        model.eval()
        return model
    
    # Function to convert STFT to time-domain signal
    def stft_to_audio(stft, sr, hop_length=512, n_fft=2048):
        stft_complex = stft[0] + 1j * stft[1]
        audio = librosa.istft(stft_complex, hop_length=hop_length)
        return audio
        
    def generate_effect_tone(clean_audio_path, generator, device, sr=22050, hop_length=512, n_fft=2048):
        clean_audio, _ = librosa.load(clean_audio_path, sr=sr)
        
        # Compute STFT
        clean_stft = librosa.stft(clean_audio, n_fft=n_fft, hop_length=hop_length)
        clean_stft = np.stack((clean_stft.real, clean_stft.imag), axis=0)
        clean_stft = torch.tensor(clean_stft, dtype=torch.float32).unsqueeze(0).to(device)
    
    generator_model_path = 'generator.pth'
    clean_audio_path = '/content/drive/MyDrive/project/project1/sample/sound samples 1/Clean/Clean/Bridge/1-13.wav'
    
    # Load the generator model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load the generator model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    generator = load_generator(generator_model_path, device)
    
    # Generate effect tone
    effect_audio = generate_effect_tone(clean_audio_path, generator, device)
    
    # Save the generated effect audio
    librosa.output.write_wav('generated_effect_audio.wav', effect_audio, sr=22050)
    ```
    
- result
- ìˆ˜ì •ì‚¬í•­

ì˜ˆìƒ ëŸ°íƒ€ì„ì´ 18ì‹œê°„ì´ë‹¤â€¦ì´ê±° ì–´ë–»ê²Œ ëŒë ¤ì•¼í• ì§€ ì¢€ ê³ ë¯¼í•´ë´ì•¼ê² ë‹¤â€¦